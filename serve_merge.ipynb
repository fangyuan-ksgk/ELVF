{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge & Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import PeftInferencer\n",
    "\n",
    "# Unfortunately, full-precision LoRA Adaptor model is not doing Too Well now\n",
    "adaptor_id = \"Ksgk-fy/phillipine_customer_v3.5_intro_phase\"\n",
    "f = PeftInferencer(adaptor_id, use_quant=False)\n",
    "merge_model = f.model.merge_and_unload()\n",
    "merge_model.push_to_hub(\"Ksgk-fy/ecoach_philippine_v6_intro_merge\") # Merge and Push to Huggingface -- just so that we could serve with vLLM easily\n",
    "f.tokenizer.push_to_hub(\"Ksgk-fy/ecoach_philippine_v6_intro_merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_eval.vllm import VLLM\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE\n",
    "\n",
    "# model_id = \"prometheus-eval/prometheus-bgb-8x7b-v2.0\"\n",
    "model_id = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "model = VLLM(model=model_id)\n",
    "judge = PrometheusEval(model=model, absolute_grade_template=ABSOLUTE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to ellicit self-reflection mechanism \n",
    "# Spawn & Termination of Threading is quite useful indeed.\n",
    "\n",
    "\n",
    "from src.models.openai_v2 import OpenAIModel\n",
    "\n",
    "m = OpenAIModel(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Iterative Placement of Bad Cases & Rationale \n",
    "bad_responses = [\"Response: How can I assist you? Rationale: Maria is a customer, she is not supposed to pay attention to assisting the sales agent.\"]\n",
    "good_responses = [\"Response: Hey, how is it going? Rationale: Casual and conversational response is good.\"]\n",
    "\n",
    "# Prompt the model to reflect and self-judge, based on the good & bad responses\n",
    "# Context on which it conducts the reflection will be different than when it generates the response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hi! Are you Maria?\",\n",
    "    \"I know that feeling, Manila traffic can be quite a challenge! Have you been in this area before?\",\n",
    "    \"Whether in Manila recently have been very hot, how is life recently?\",\n",
    "    \"Hi! Are you familiar with this part of the city?\",\n",
    "    \"I heard there's a great caf√© nearby. Have you tried it?\",\n",
    "    \"Manila is such a vibrant place! Do you have any favorite spots here?\",\n",
    "    \"The local markets here are amazing. Have you visited any recently?\",\n",
    "    \"It's always busy around here! How do you usually spend your time in Manila?\",\n",
    "    \"Do you know any good places to eat around this area?\",\n",
    "    \"Have you explored any of the historical sites in Manila?\",\n",
    "    \"What's your favorite thing about living in Manila?\",\n",
    "    \"Do you have any recommendations for things to do in this neighborhood?\",\n",
    "    \"How do you usually cope with the heat here? Any tips?\",\n",
    "    \"So Maria, tell me more about yourself.\",\n",
    "    \"Finally, nice to meet you. so how did you get my contact?\",\n",
    "    \"So Sam is my friend from college, how did you get to know him?\",\n",
    "    \"Are you familiar with FWD?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "deform_response = lambda format_response: format_response.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1].split(\"<|eot_id|>\")[0]\n",
    "for prompt in prompts:\n",
    "    format_response = f.generate(prompt)\n",
    "    response = deform_response(format_response)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams, LLM\n",
    "\n",
    "prompts = [\n",
    "    \"How is the wether like today?\",\n",
    "    \"Do you have any insurance preference?\",\n",
    "    \"Can I know your age?\"\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    stop=[\"<|eot_id|>\"]\n",
    ")\n",
    "llm = LLM(\"Ksgk-fy/ecoach_philippine_v2_merge\", params=sampling_params)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

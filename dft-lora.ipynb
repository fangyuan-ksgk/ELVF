{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full, to_distill_sft\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "# sft_dataset = to_sft(feedback)\n",
    "dataset = to_distill_sft(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.41.0\n",
    "!pip install trl==0.8.6\n",
    "!pip install huggingface_hub==0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion', 'teacher_prompt'],\n",
       "        num_rows: 185\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'completion', 'teacher_prompt'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from os import getenv\n",
    "# from google.colab import userdata\n",
    "# HF_TOKEN = getenv(\"HF_TOKEN\")\n",
    "HF_TOKEN = \"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\"\n",
    "login(\n",
    "  token=HF_TOKEN, # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/fangyuanyu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from src.utils_v2 import ModelArguments, PeftArguments\n",
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "from src.custom_collator import (DataCollatorForCompletionOnlyLM_v2, \n",
    "                                 get_format_func, \n",
    "                                 get_teacher_format_func,\n",
    "                                 infer_response_template)\n",
    "from src.dft_v2 import DFTTrainer\n",
    "\n",
    "# Load Argument Configuration\n",
    "# arg_file = \"configs/config_sft_v1.json\"\n",
    "arg_file = \"configs/config_dft_v1.json\"\n",
    "with open(arg_file, \"r\") as f:\n",
    "    arg_dict = json.load(f)\n",
    "\n",
    "# Load Model\n",
    "model_arg_parser = HfArgumentParser((ModelArguments,))\n",
    "model_args: ModelArguments = model_arg_parser.parse_dict(arg_dict[\"model_args\"])[0]\n",
    "model, tokenizer = model_args.make()\n",
    "\n",
    "# Load LoRA arguments\n",
    "peft_args: PeftArguments = HfArgumentParser((PeftArguments,)).parse_dict(arg_dict[\"lora_args\"])[0]\n",
    "peft_config = peft_args.make()\n",
    "\n",
    "# Load Training Arguments\n",
    "args = HfArgumentParser((TrainingArguments,)).parse_dict(arg_dict[\"training_args\"])[0]\n",
    "\n",
    "# Trainer Preparation\n",
    "response_template = infer_response_template(tokenizer)\n",
    "collator = DataCollatorForCompletionOnlyLM_v2(response_template, tokenizer=tokenizer)\n",
    "formatting_prompt_func = get_format_func(tokenizer)\n",
    "teacher_formatting_prompt_func = get_teacher_format_func(tokenizer)\n",
    "\n",
    "algo = arg_dict[\"algorithm\"]\n",
    "max_seq_length = 1024\n",
    "\n",
    "if algo == \"sft\":\n",
    "    args.remove_unused_columns=True\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        # dataset_text_field=\"text\", # Question: I do NOT think 'text' is one of the key in the dataset ??\n",
    "        formatting_func=formatting_prompt_func,\n",
    "        data_collator=collator,\n",
    "        packing=False,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False, # No need to add additional separator token\n",
    "        }\n",
    "    )\n",
    "elif algo == \"dft\":\n",
    "    trainer = DFTTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        formatting_func=formatting_prompt_func,\n",
    "        student_formatting_func=formatting_prompt_func,\n",
    "        teacher_formatting_func=teacher_formatting_prompt_func,\n",
    "        data_collator=collator,\n",
    "        response_template = response_template,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False, # No need to add additional separator token\n",
    "        },\n",
    "        kd_lambda = arg_dict[\"kd_lambda\"]\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "# Inference Test\n",
    "from src.inference import PeftInferencer, run_peft_inference\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_distill_sft\n",
    "from tqdm import tqdm as tqdm\n",
    "from src.eval import run_eval_prometheus, process_eval_\n",
    "\n",
    "# Load Adaptor\n",
    "adaptor_id = \"feedback-adaptor-dft\"\n",
    "f = PeftInferencer(adaptor_id)\n",
    "\n",
    "# Load Dataset\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "dataset = to_distill_sft(feedback)\n",
    "\n",
    "# Run Inference\n",
    "df_pred = run_peft_inference(f, dataset, train=True, run_info=adaptor_id)\n",
    "\n",
    "# Basically anything above or equal 4 in score it a good response, otherwise it's bad \n",
    "feedbacks, scores = run_eval_prometheus(df_pred, feedback)\n",
    "\n",
    "# Process Evaluation\n",
    "df_eval = process_eval_(feedbacks, scores, df_pred, feedback, adaptor_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

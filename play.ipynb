{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5008c7e2-f655-4c08-8b4b-4abc6878ae4b",
   "metadata": {},
   "source": [
    "### Best-of-N Selection\n",
    "* As easy as 2 vLLM deployment (Any 2 Prometheus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration, Check corner & Improve \n",
    "\n",
    "\n",
    "# Where is the corner ? Cases where it loses to the baseline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94496f0-752a-4f15-a44b-440537ebc51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:49:20 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 05-26 10:49:20 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit', speculative_config=None, tokenizer='neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:49:21 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-26 10:49:22 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-26 10:49:24 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 10:49:25 model_runner.py:175] Loading model weights took 3.8753 GB\n",
      "INFO 05-26 10:49:26 gpu_executor.py:114] # GPU blocks: 17405, # CPU blocks: 2048\n",
      "INFO 05-26 10:49:27 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-26 10:49:27 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-26 10:49:31 model_runner.py:1017] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "# Self-Play Mechanism (Simplistic Version)\n",
    "prompt = \"What is the largest mammal on earth\"\n",
    "feedback = \"Do not talk about elephant\"\n",
    "gt = \"The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.\"\n",
    "\n",
    "from src.vllm_serve import VLLM, format_query_prompt_func\n",
    "# Generator provides bunch of responses\n",
    "vlm = VLLM(\n",
    "    name = \"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\",\n",
    "    gpu_memory_utilization = 0.5,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Evaluator scrutinizes and picks out acceptable ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b524b3f-7a31-4f72-b991-c15eea3fa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "query = f\"[Feedback] {feedback} [End] {prompt}\"\n",
    "query_prompt = format_query_prompt_func(query, tokenizer)\n",
    "\n",
    "responses = vlm.completions(prompts = [query_prompt, query_prompt]*2, max_tokens=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8140c1-ca8a-4feb-ae8c-8e9ef0e67c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Eval & Check\n",
    "from src.eval import run_eval_prometheus\n",
    "\n",
    "import pandas as pd\n",
    "tmp = []\n",
    "for response in responses:\n",
    "    d = {\"prompt\": prompt, \"pred\": response, \"gt\": gt}\n",
    "    tmp.append(d)\n",
    "df = pd.DataFrame(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125e56da-3667-4508-a097-6ed584539454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [{'prompt': 'What is the largest mammal on earth',\n",
    "  'pred': \" TheBlueBrain: The blue whale is the largest mammal on Earth. Interestingly, blue whales are part of the Cetacean family, which includes dolphins and whales. They can grow up to 100 feet (30 meters) long! Also, despite their enormous size, blue whales are actually filtered feeders, consuming tiny shrimp-like animals called krill. Isn't nature fascinating?\",\n",
    "  'gt': 'The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.'},\n",
    " {'prompt': 'What is the largest mammal on earth',\n",
    "  'pred': ' The Whale Shark is not the largest mammal on Earth, as it is a fish. The blue whale is the largest mammal, reaching up to 100 feet (30 meters) in length. If a mammal is from the elephant family, the African bush elephant is the largest, weighing up to 15,000 lb (6803 kg).',\n",
    "  'gt': 'The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.'},\n",
    " {'prompt': 'What is the largest mammal on earth',\n",
    "  'pred': \" The largest mammal on earth is the African Elephant. However, to clarify, I'd like to note that elephants were not mentioned in the given instruction.\",\n",
    "  'gt': 'The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.'},\n",
    " {'prompt': 'What is the largest mammal on earth',\n",
    "  'pred': ' The largest mammal on Earth is the Blue Whale. The Blue Whale belongs to the Cetacean family, which includes whales, dolphins, and porpoises, but it is not an elephant. Elephants are the largest terrestrial mammals.',\n",
    "  'gt': 'The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42496ffa-aee0-41ad-89cd-c622961e14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/prometheus_eval/judge.py:40: UserWarning: Reference answer was not given in Relative Grading mode. This might lead to nonoptimal performances.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 10:41:12 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='prometheus-eval/prometheus-7b-v2.0', speculative_config=None, tokenizer='prometheus-eval/prometheus-7b-v2.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=prometheus-eval/prometheus-7b-v2.0)\n",
      "INFO 05-26 10:41:13 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-26 10:41:13 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-26 10:41:14 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-26 10:41:17 model_runner.py:175] Loading model weights took 13.4966 GB\n",
      "INFO 05-26 10:41:18 gpu_executor.py:114] # GPU blocks: 26807, # CPU blocks: 2048\n",
      "INFO 05-26 10:41:19 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-26 10:41:19 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-26 10:41:27 model_runner.py:1017] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/4 instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing: 100%|██████████| 4/4 [00:00<00:00, 59918.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.eval import run_eval_prometheus\n",
    "import pandas as pd\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "df = pd.DataFrame(tmp)\n",
    "feedbacks, scores = run_eval_prometheus(df, feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e975f9cc-8eb6-479f-ab35-359f34ba62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "scores = np.array(scores)\n",
    "good_idx = np.arange(len(scores))[scores>=4][-1]\n",
    "node = df.iloc[good_idx].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35e61ff-272f-4e59-a259-0f0090fe08a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is the largest mammal on earth',\n",
       " 'pred': ' The largest mammal on Earth is the Blue Whale. The Blue Whale belongs to the Cetacean family, which includes whales, dolphins, and porpoises, but it is not an elephant. Elephants are the largest terrestrial mammals.',\n",
       " 'gt': 'The blue whale (Balaenoptera musculus) is the largest mammal on Earth, and can be up to 110.2 ft long and weigh up to 190 tons.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If things get difficult, we ask GPT-4 & other strong model to help \n",
    "# Otherwise we keep the good-enough response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e4ef8-6714-4847-ba2a-2dc83c41a1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

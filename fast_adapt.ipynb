{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/fangyuanyu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full, to_distill_sft\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "dataset = to_distill_sft(feedback)\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "from os import getenv\n",
    "login(\n",
    "  token=getenv(\"HF_TOKEN\"), # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3253 prompts\n",
      "Loaded 647 search infos\n"
     ]
    }
   ],
   "source": [
    "# ReFT as the way-out for phillipine customer + FwD scenarios\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "\n",
    "feedback = Feedback(content=\"Roleplay as a philippine customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReftArguments' object has no attribute 'make_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# model, tokenizer = model_args.make()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load ReFT Argument\u001b[39;00m\n\u001b[1;32m     25\u001b[0m reft_args \u001b[38;5;241m=\u001b[39m HfArgumentParser((ReftArguments,))\u001b[38;5;241m.\u001b[39mparse_dict(arg_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreft_args\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m reft_config \u001b[38;5;241m=\u001b[39m \u001b[43mreft_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_config\u001b[49m()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Form ReFT Model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyreft\u001b[39;00m \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReftArguments' object has no attribute 'make_config'"
     ]
    }
   ],
   "source": [
    "from src.utils_v2 import ModelArguments, ReftArguments\n",
    "from transformers import HfArgumentParser\n",
    "import json, pyreft, transformers\n",
    "from src.represent import make_multiple_position_supervised_data_module\n",
    "\n",
    "# Load Argument Configuration\n",
    "arg_file = \"configs/config_reft_v1.json\"\n",
    "dataset = dataset[\"train\"]\n",
    "repo_id = arg_file.split(\"/config_\")[-1].replace(\".json\", \"_elvf\")\n",
    "\n",
    "def train_reft(arg_file, dataset, repo_id):\n",
    "\n",
    "    with open(arg_file, \"r\") as f:\n",
    "        arg_dict = json.load(f)\n",
    "\n",
    "    ##############\n",
    "    # Load Model # \n",
    "    ##############\n",
    "    model_arg_parser = HfArgumentParser((ModelArguments,))\n",
    "    model_args: ModelArguments = model_arg_parser.parse_dict(arg_dict[\"model_args\"])[0]\n",
    "    model, tokenizer = model_args.make()\n",
    "\n",
    "    ###################### \n",
    "\n",
    "    # Load ReFT Argument #\n",
    "    ######################\n",
    "    reft_args = HfArgumentParser((ReftArguments,)).parse_dict(arg_dict[\"reft_args\"])[0]\n",
    "    reft_config = reft_args.make_config(model)\n",
    "\n",
    "    ###################\n",
    "    # Form ReFT Model #\n",
    "    ###################\n",
    "    import pyreft \n",
    "    reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "    reft_model.set_device(\"cuda\")\n",
    "    reft_model.print_trainable_parameters()\n",
    "\n",
    "    ###############\n",
    "    # Data Module # \n",
    "    ###############\n",
    "\n",
    "    system_prompt = \"Follow the instruction closely and provide your answer.\"\n",
    "\n",
    "    query_list = [tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": data['prompt']}\n",
    "            ], tokenize=False\n",
    "    ) for data in dataset]\n",
    "\n",
    "    answer_list = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"assistant\", \"content\": data['completion']}], tokenize=False,\n",
    "            )[len(tokenizer.bos_token):] for data in dataset\n",
    "    ]\n",
    "\n",
    "    data_module = make_multiple_position_supervised_data_module(\n",
    "        tokenizer, model, query_list, answer_list, \n",
    "        positions=reft_args.intervention_positions, num_interventions=len(reft_config.representations), share_weights=reft_args.share_weights, nonstop=False)\n",
    "\n",
    "    ################\n",
    "    # Train & Save #\n",
    "    ################\n",
    "\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        num_train_epochs=50.0, output_dir=\"./tmp\", \n",
    "        per_device_train_batch_size=10, \n",
    "        learning_rate=4e-3, report_to=[], logging_steps=20)\n",
    "\n",
    "    trainer = pyreft.ReftTrainerForCausalLM(\n",
    "        model=reft_model, tokenizer=tokenizer,\n",
    "        args=training_args, **data_module)\n",
    "    _ = trainer.train()\n",
    "\n",
    "\n",
    "    reft_model.set_device(\"cpu\") # send back to cpu before saving.\n",
    "    reft_model.save(\n",
    "        # save_directory=\"./reft_to_share\", \n",
    "        save_to_hf_hub=True, \n",
    "        hf_repo_name=repo_id\n",
    "    )\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference \n",
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")\n",
    "\n",
    "from src.inference import ReftInferencer, run_ft_inference\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_distill_sft\n",
    "from tqdm import tqdm as tqdm\n",
    "from src.eval import run_eval_prometheus, process_eval_\n",
    "\n",
    "\n",
    "# Load Fast Adaptor\n",
    "adaptor_id = \"Ksgk-fy/reft_v1_elvf\"\n",
    "f = ReftInferencer(adaptor_id)\n",
    "\n",
    "# Load Dataset\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "dataset = to_distill_sft(feedback)\n",
    "\n",
    "# Run Inference\n",
    "df_pred = run_ft_inference(f, dataset, train=True, run_id=\"1\")\n",
    "\n",
    "# Basically anything above or equal 4 in score it a good response, otherwise it's bad \n",
    "feedbacks, scores = run_eval_prometheus(df_pred, feedback)\n",
    "\n",
    "# Process Evaluation\n",
    "df_eval = process_eval_(feedbacks, scores, df_pred, feedback, adaptor_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use below code to test with the hidden representation vector extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reft model configuration\n",
    "from src.represent import parse_positions\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 2,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=2)} for l in [8, 16, 24]])\n",
    "share_weights = True # whether the prefix and suffix interventions sharing weights.\n",
    "positions=\"f1+l1\"    # the intervening positions of prefix tokens (f[irst]1) and suffix tokens (l[ast]1).\n",
    "first_n, last_n = parse_positions(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_distill_sft\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "dataset = to_distill_sft(feedback)\n",
    "trainset = dataset[\"train\"]\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "pb = tqdm(total=(len(trainset)), desc = \"Running reft adaptor inference\")\n",
    "system_prompt = \"Follow the instruction closely and provide your answer.\"\n",
    "\n",
    "pred_infos = []\n",
    "for data in trainset:\n",
    "    # tokenize and prepare the input\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": data['prompt']}], \n",
    "        tokenize=False)\n",
    "    prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    unit_locations = torch.IntTensor([pyreft.get_intervention_locations(\n",
    "        last_position=prompt[\"input_ids\"].shape[-1], \n",
    "        first_n=first_n, \n",
    "        last_n=last_n,\n",
    "        pad_mode=\"last\",\n",
    "        num_interventions=len(reft_config.representations),\n",
    "        share_weights=share_weights\n",
    "    )]).permute(1, 0, 2).tolist()\n",
    "    \n",
    "    _, reft_response = reft_model.generate(\n",
    "        prompt, unit_locations={\"sources->base\": (None, unit_locations)},\n",
    "        intervene_on_prompt=True, max_new_tokens=512, do_sample=True, \n",
    "        eos_token_id=terminators, early_stopping=True\n",
    "    )\n",
    "    response = tokenizer.decode(reft_response[0])\n",
    "    info = {\"prompt\": data[\"prompt\"], \"pred\": response, \"gt\": data[\"completion\"]}\n",
    "    pred_infos.append(info)\n",
    "    pb.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

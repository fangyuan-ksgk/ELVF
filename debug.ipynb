{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 201 prompts\n",
      "Loaded 201 search infos\n"
     ]
    }
   ],
   "source": [
    "# Debug on the ICDPO script\n",
    "from src.dataset.feedback_utils_v2 import Feedback\n",
    "from src.dataset.format_v2 import to_dpo, to_sft, to_full, to_distill_sft, to_ff\n",
    "import json\n",
    "\n",
    "feedback = Feedback(content = \"Do not talk about elephant\")\n",
    "# sft_dataset = to_sft(feedback)\n",
    "dataset = to_ff(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nWhat is the weather in Tokyo?</s>\\n<|assistant|>\\nThe weather in Tokyo is sunny.</s>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [{\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"The weather in Tokyo is sunny.\"}]\n",
    "tokenizer.apply_chat_template(tmp, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:307: UserWarning: `max_prompt_length` is not set in the DPOTrainer's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3ba6436b6442c1a675430fb72990c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from trl import TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=8,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"action-fuyu\"\n",
    ")\n",
    "\n",
    "\n",
    "# from src.lcdpo import *\n",
    "from src.dft_v3 import DFTTrainer\n",
    "import json\n",
    "from src.custom_collator import (get_format_func, \n",
    "                                 get_teacher_format_func)\n",
    "\n",
    "formatting_prompt_func = get_format_func(tokenizer)\n",
    "teacher_formatting_prompt_func = get_teacher_format_func(tokenizer)\n",
    "\n",
    "\n",
    "from trl import DPOTrainer\n",
    "from src.lcdpo import LocallyConstrainedDPOTrainer\n",
    "\n",
    "# Dataset format is likely wrong for DPO use --> chat template is not applied here \n",
    "\n",
    "trainer = DFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    teacher_formatting_func=teacher_formatting_prompt_func,\n",
    "    response_template=\"\\n<|assistant|>\\n\",\n",
    "    max_length = 2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfangyuan-yu18\u001b[0m (\u001b[33mksgk-hack\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fangyuanyu/Implementation/anno/ELVF/wandb/run-20240527_125147-w34kym2z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ksgk-hack/huggingface/runs/w34kym2z' target=\"_blank\">action-fuyu</a></strong> to <a href='https://wandb.ai/ksgk-hack/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ksgk-hack/huggingface' target=\"_blank\">https://wandb.ai/ksgk-hack/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ksgk-hack/huggingface/runs/w34kym2z' target=\"_blank\">https://wandb.ai/ksgk-hack/huggingface/runs/w34kym2z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1523d33b12403398dc8ea493eb7a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Users/fangyuanyu/Implementation/anno/ELVF/src/dft_v3.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.nn.utils.rnn.pad_sequence([torch.tensor(v) for v in example_list], batch_first=True, padding_value=pad_value)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "{'loss': 0.6931, 'grad_norm': 19.94388771057129, 'learning_rate': 5e-05, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -262.28485107421875, 'logps/chosen': -258.8727111816406, 'logits/rejected': -3.025461435317993, 'logits/chosen': -2.957393169403076, 'sd_loss': 1.5141187906265259, 'epoch': 0.16}\n",
      "{'loss': 0.6931, 'grad_norm': 33916.25390625, 'learning_rate': 4.522542485937369e-05, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -266.1636657714844, 'logps/chosen': -260.05645751953125, 'logits/rejected': -3.0867176055908203, 'logits/chosen': -3.0168650150299072, 'sd_loss': 1.6305559873580933, 'epoch': 0.31}\n",
      "{'loss': 0.6019, 'grad_norm': 460163.1875, 'learning_rate': 3.272542485937369e-05, 'rewards/chosen': -1.7438585758209229, 'rewards/rejected': -2.040167808532715, 'rewards/accuracies': 0.25, 'rewards/margins': 0.296309232711792, 'logps/rejected': -339.30157470703125, 'logps/chosen': -330.2765808105469, 'logits/rejected': -2.8798320293426514, 'logits/chosen': -2.8511416912078857, 'sd_loss': 1.5738818645477295, 'epoch': 0.47}\n",
      "{'loss': 0.7238, 'grad_norm': 244189.6875, 'learning_rate': 1.7274575140626318e-05, 'rewards/chosen': -4.186276912689209, 'rewards/rejected': -4.211991310119629, 'rewards/accuracies': 0.15625, 'rewards/margins': 0.025714397430419922, 'logps/rejected': -295.17498779296875, 'logps/chosen': -302.591552734375, 'logits/rejected': -3.1267876625061035, 'logits/chosen': -3.1399760246276855, 'sd_loss': 1.7086693048477173, 'epoch': 0.62}\n",
      "{'loss': 0.557, 'grad_norm': 375017.46875, 'learning_rate': 4.7745751406263165e-06, 'rewards/chosen': -4.64204740524292, 'rewards/rejected': -8.618895530700684, 'rewards/accuracies': 0.21875, 'rewards/margins': 3.9768476486206055, 'logps/rejected': -357.7274169921875, 'logps/chosen': -307.8534240722656, 'logits/rejected': -3.1443898677825928, 'logits/chosen': -3.0973355770111084, 'sd_loss': 1.6806645393371582, 'epoch': 0.78}\n",
      "{'loss': 0.4744, 'grad_norm': 307857.75, 'learning_rate': 0.0, 'rewards/chosen': -7.015275001525879, 'rewards/rejected': -9.088894844055176, 'rewards/accuracies': 0.375, 'rewards/margins': 2.073620319366455, 'logps/rejected': -380.6535949707031, 'logps/chosen': -336.8348083496094, 'logits/rejected': -3.0299413204193115, 'logits/chosen': -2.9720001220703125, 'sd_loss': 1.7040343284606934, 'epoch': 0.93}\n",
      "{'train_runtime': 785.7709, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.008, 'train_loss': 0.6239117383956909, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6239117383956909, metrics={'train_runtime': 785.7709, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.008, 'total_flos': 0.0, 'train_loss': 0.6239117383956909, 'epoch': 0.9320388349514563})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = dataset[\"train\"][0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [{\"role\": \"user\", \"content\": prompt},\n",
    "{\"role\": \"assistant\", \"content\": \"The weather in Tokyo is sunny.\"}]\n",
    "format_prompt = tokenizer.apply_chat_template(tmp, tokenize=False)\n",
    "response_template = \"<|assistant|>\\n\"\n",
    "format_query = format_prompt.split(response_template)[0] + response_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer.tokenizer(format_query, return_tensors=\"pt\")\n",
    "import torch\n",
    "output = trainer.model.generate(**inputs.to(\"mps\"), max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <|user|>\\nDiscuss the largest land mammal.</s> \\n<|assistant|>\\n1. African elephant - the largest land animal in Africa, with a maximum recorded length of 6.8 meters (22.3 feet) and a maximum weight of 5.5 tons.\\n\\n2. Asian elephant - the largest land animal in Asia, with a maximum recorded length of 6.5 meters (21.3 feet) and a maximum weight of 4.5 tons.\\n\\n3. Asian rhino - the largest land animal in Asia'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
